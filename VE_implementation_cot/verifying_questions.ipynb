{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "from dataset_utils import read_wikiqa_data\n",
    "# from comp_utils import safe_completion\n",
    "from prompt_helper import get_joint_prompt_helper\n",
    "# import consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MAX_TOKENS = 70\n",
    "\n",
    "# PROMOT CONTROL\n",
    "EP_STYLE_SEP = \" The answer is\"\n",
    "EP_POSSIBLE_SEP_LIST = [\n",
    "    \" The answer is\",\n",
    "    \" First, the answer is\",\n",
    "    \" Second, the answer is\",\n",
    "    \" Third, the answer is\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_args():\n",
    "    '''\n",
    "    Function that parses arguments passed to the script\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser()\n",
    "    add_engine_argumenet(parser)\n",
    "\n",
    "    parser.add_argument('--style', type=str, default=\"e-p\")\n",
    "    parser.add_argument('--annotation', type=str, default=\"std\")\n",
    "    parser.add_argument('--run_prediction', default=False, action='store_true')\n",
    "    parser.add_argument('--num_shot', type=int, default=6)\n",
    "    parser.add_argument('--train_slice', type=int, default=0)\n",
    "    parser.add_argument('--num_dev', type=int, default=1000)\n",
    "    parser.add_argument('--dev_slice', type=int, default=0)\n",
    "    parser.add_argument('--show_result',  default=False, action='store_true')\n",
    "    parser.add_argument('--model', type=str, default=\"gpt3\")\n",
    "    parser.add_argument('--temperature', type=float, default=0.7)\n",
    "    parser.add_argument('--consistency_threshold', type=float, default=0.5)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    specify_engine(args)\n",
    "    args.helper = get_joint_prompt_helper(args.style)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_cache_name(args):\n",
    "    return \"misc/verifying_questions_tr{}-{}_dv{}-{}_thres{}_temp_{}.json\".format( \\\n",
    "        args.train_slice, args.train_slice + args.num_shot, args.dev_slice, args.num_dev,\n",
    "        args.consistency_threshold, args.temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_context_manual_prediction(question, sentence, engine, model, helper):\n",
    "    prompt, stop_signal = helper.prompt_for_question_generation(question, sentence)\n",
    "    if model == 'gpt3':\n",
    "        pred = safe_completion(engine, prompt, _MAX_TOKENS, stop_signal, n = 1, temp=0.0, logprobs=5)        \n",
    "        if pred != None:\n",
    "            if len(pred[\"text\"]) > len(prompt):\n",
    "                pred[\"text\"] = pred[\"text\"][len(prompt):]\n",
    "            else:\n",
    "                pred[\"text\"] = \"null\"\n",
    "            pred[\"completion_offset\"] = len(prompt)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_manual_predictions(dev_set, predictions, verifying_questions, args, do_print=False):\n",
    "    num = 0\n",
    "    for idx, (ex, pred) in enumerate(zip(dev_set, predictions)):\n",
    "        if pred['consistency'] < args.consistency_threshold:\n",
    "            num += 1\n",
    "            id = ex['id']\n",
    "            for c in verifying_questions:\n",
    "                if c['id']==id:\n",
    "                    cont = c['verifying_questions']\n",
    "                    break\n",
    "            if do_print:\n",
    "                print(\"--------------{} EX {} CONS--------------\".format(idx, pred['consistency']))\n",
    "                print('question: ', ex['question'])\n",
    "                sentences = rationale_tokenize(pred['rationale'])\n",
    "\n",
    "                for j, s in enumerate(sentences):\n",
    "                    print('rationale_sentence {}: {}'.format(j, s))\n",
    "                    print('verifying_question {}: {}'.format(j, cont[j]))\n",
    "    print(f'{num} instances below consistency threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_few_shot_manual_prediction(args):\n",
    "    print(\"Running prediction\")\n",
    "    train_set = read_wikiqa_data(f\"data/train_subset.json\", manual_annotation_style=args.style)\n",
    "    train_set = train_set[args.train_slice:(args.train_slice + args.num_shot)]\n",
    "    print('len(train_set): ', len(train_set))\n",
    "    dev_set = read_wikiqa_data(f\"data/dev_sampled.json\")\n",
    "    dev_set = dev_set[args.dev_slice:(args.num_dev)]\n",
    "\n",
    "    prompt, _ = args.helper.prompt_for_question_generation('question', 'sentence')\n",
    "    print('prompt: ')\n",
    "    print(prompt)\n",
    "\n",
    "    # finished consistency and processs\n",
    "    print('args.num_dev: ', args.num_dev)\n",
    "    predictions = read_full(args, consistency)\n",
    "    new_predictions, cons = [], []\n",
    "    for i, p in enumerate(tqdm(predictions, total=len(predictions), desc=\"Verifying\")):\n",
    "        ex = dev_set[i]\n",
    "        con, new_p = consistency.post_process_consistency(ex, p, args)\n",
    "        cons.append(con)\n",
    "        new_predictions.append(new_p)\n",
    "    predictions = new_predictions \n",
    "    [args.helper.post_process(p) for p in predictions] \n",
    "\n",
    "    if os.path.exists(result_cache_name(args)):\n",
    "        # finished all steps, evaluating\n",
    "        verifying_questions = read_json(result_cache_name(args))\n",
    "        print(result_cache_name(args))\n",
    "    else:\n",
    "        print('running verifying question generation')\n",
    "        verifying_questions = []\n",
    "        for i, p in enumerate(tqdm(predictions, total=len(predictions), desc=\"Verifying\")):\n",
    "            ex = dev_set[i]\n",
    "            con = p['consistency']\n",
    "            if con < args.consistency_threshold:\n",
    "                vq = []\n",
    "                sentences = rationale_tokenize(p['rationale'])\n",
    "                for q, s in enumerate(sentences):\n",
    "                    question = in_context_manual_prediction(ex['question'], s, args.engine, args.model, args.helper)\n",
    "                    if question != None:\n",
    "                        vq.append(question['text'])\n",
    "                    else:\n",
    "                        args.num_dev = i + args.dev_slice\n",
    "                        dump_json(verifying_questions, result_cache_name(args)) \n",
    "                        print(result_cache_name(args))\n",
    "                        raise Exception('end')\n",
    "                vq = {'id': ex['id'], 'verifying_questions': vq}\n",
    "                verifying_questions.append(vq)\n",
    "        # save\n",
    "        dump_json(verifying_questions, result_cache_name(args)) \n",
    "        \n",
    "    evaluate_manual_predictions(dev_set, predictions, verifying_questions, args, do_print=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
